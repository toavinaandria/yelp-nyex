{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T17:48:29.785445",
     "start_time": "2017-01-17T17:48:29.329237"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import _pickle\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gc\n",
    "import logging\n",
    "import nltk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis looks at predicting restaurant ratings from review text primarily.\n",
    "\n",
    "Note: Inspired and with code snippets from https://www.kaggle.com/c/word2vec-nlp-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T17:48:55.499626",
     "start_time": "2017-01-17T17:48:55.323640"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#1. Set variables\n",
    "input_folder = join('pickles','3.agg_dfs')\n",
    "save_folder = join('pickles','4.restaurant_text_predictions')\n",
    "stopwords_path = join('pickles','useful_objects','eng_stopwords.pkl')\n",
    "\n",
    "# Register TQDM (progress bar) with Pandas to see progress for apply functions\n",
    "tqdm_notebook().pandas(desc='Progress Bar')\n",
    "\n",
    "# Load stopwords pickle (originally from NLTK downloads)\n",
    "stopwords = _pickle.load(open(stopwords_path,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load data and filter for restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T01:05:09.582875",
     "start_time": "2017-01-17T01:04:51.727907"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. Load data\n",
    "business_data = _pickle.load(open(join(input_folder,'business_data.pkl'),'rb'))\n",
    "review_data = _pickle.load(open(join(input_folder,'review_data.pkl'),'rb'))\n",
    "\n",
    "\n",
    "# 3. Filter restaurants only for business data and reviews\n",
    "\n",
    "restaurant_filter = business_data.categories.apply(lambda x: 'Restaurants' in x)\n",
    "\n",
    "restaurant_data = business_data[restaurant_filter]\n",
    "restaurant_ids = set(restaurant_data.business_id)\n",
    "\n",
    "restaurant_review_filter = review_data['business_id'].progress_apply(lambda x: x in restaurant_ids)\n",
    "restaurant_reviews = review_data[restaurant_review_filter]\n",
    "\n",
    "restaurant_reviews = pd.merge(restaurant_reviews,restaurant_data,\n",
    "                              how='left',\n",
    "                              left_on='business_id',\n",
    "                              right_on='business_id')\n",
    "\n",
    "# Filter for reviews in majority English-speaking states to ensure language processing appropriate\n",
    "english_speaking_states = set(['PA','NC','SC','WI','IL','AZ','CA','NV','FL','NM','ON','TX','EDH','MLN',\n",
    "                               'HAM','SCB','ELN','FIF','NTH','XGL','KHL','MN','AK'])\n",
    "\n",
    "eng_restaurant_reviews = restaurant_reviews[restaurant_reviews.state.apply(lambda x: x in english_speaking_states)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Prepare and save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T09:26:01.368316",
     "start_time": "2017-01-17T09:26:01.365011"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Import stopwords\n",
    "\n",
    "# Note - Those stopwords were initially downloaded from the NLTK English stopwords corpus\n",
    "stopwords = _pickle.load(open(stopwords_path),'rb')\n",
    "\n",
    "\n",
    "# 5. Process text\n",
    "eng_restaurant_reviews['processed_text_BoW'] = eng_restaurant_reviews['text']\n",
    "\n",
    "def clean_reviews_BoW(review_text):\n",
    "    \"\"\"Converts review text for bag of words analysis\"\"\"\n",
    "    \n",
    "    # 1. Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 3. Convert stopwords to set\n",
    "    stops = set(stopwords)\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    relevant_words = [word for word in words if not word in stops]\n",
    "    \n",
    "    # 5. Join words back\n",
    "    return (' '.join(relevant_words))\n",
    "\n",
    "# Split into training and test data\n",
    "\n",
    "perc_training_data = 0.7\n",
    "num_reviews = eng_restaurant_reviews.processed_text_BoW.size\n",
    "\n",
    "num_training_obs = int(round(perc_training_data * num_reviews, 0))\n",
    "\n",
    "eng_restaurant_reviews = eng_restaurant_reviews.reset_index()\n",
    "eng_restaurant_reviews = eng_restaurant_reviews.rename(columns={'index':'original_index'})\n",
    "\n",
    "np.random.seed(500)\n",
    "training_indices = set(np.random.choice(num_reviews, num_training_obs, replace=False))\n",
    "\n",
    "\n",
    "eng_restaurant_reviews['idx'] = eng_restaurant_reviews.index\n",
    "eng_restaurant_reviews['training_indices'] = eng_restaurant_reviews.idx.apply(lambda x: x in training_indices)\n",
    "eng_restaurant_reviews['processed_text_BoW'] = eng_restaurant_reviews['text'].progress_apply(clean_reviews_BoW)\n",
    "\n",
    "# Reduce features to reduce memory use\n",
    "eng_restaurant_reviews = eng_restaurant_reviews[['business_id','stars_x','stars_y','date','text','city','open','state',\n",
    "                                                'review_count','processed_text_BoW','training_indices']]\n",
    "_pickle.dump(eng_restaurant_reviews,open(join(save_folder,'eng_restaurant_reviews.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Prepare Features to Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-17T12:34:48.637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note - You start from here if needed as this loads the relevant file from the previous step\n",
    "eng_restaurant_reviews = _pickle.load(open(join(save_folder,'eng_restaurant_reviews.pkl'),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-17T12:35:09.184Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def reduce_ratings(series):\n",
    "    \"\"\"\n",
    "    Reduces ratings to 0: Below average (1 or 2), 1: Average (3) or Above Average (4 and 5)\n",
    "    as this is sufficient accuracy to gauge business quality\n",
    "    \"\"\"\n",
    "    ratings_3 = {1:0, 2:0, 3:1, 4:2, 5:2}\n",
    "    series = ratings_3[series]\n",
    "    return series\n",
    "\n",
    "def run_model(name,model):\n",
    "    \"\"\"\n",
    "    Runs sci-kitlearn classification models and provides accuracy as a metric\n",
    "    \"\"\"\n",
    "    print('Fitting {} model'.format(name))\n",
    "    model.fit(BoW_X_train,BoW_Y_train)\n",
    "    print('Model {} has finished training'.format(name))\n",
    "    print('The {} model has an accuracy of {} \\n'.format(name, model.score(BoW_X_test,BoW_Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T01:05:25.992171",
     "start_time": "2017-01-17T01:05:21.653834"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorizer limited to 5,000 most common words for memory constraints and as additional features do not\n",
    "# increase performance significantly \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = None,\n",
    "                            max_features = 5000)\n",
    "    \n",
    "\n",
    "# Prepare training and test data\n",
    "eng_restaurant_reviews['reduced_rating'] = eng_restaurant_reviews.stars_x.progress_apply(reduce_ratings)\n",
    "eng_restaurant_reviews_training = eng_restaurant_reviews[eng_restaurant_reviews['training_indices'] == True]\n",
    "eng_restaurant_reviews_test = eng_restaurant_reviews[eng_restaurant_reviews['training_indices'] == False]\n",
    "\n",
    "BoW_training_data_features = vectorizer.fit_transform(eng_restaurant_reviews_training.processed_text_BoW)\n",
    "\n",
    "BoW_X_test = vectorizer.transform(eng_restaurant_reviews_test.processed_text_BoW)\n",
    "BoW_Y_test = np.array(eng_restaurant_reviews_test.reduced_rating)\n",
    "\n",
    "BoW_X_train = BoW_training_data_features\n",
    "BoW_Y_train = np.array(eng_restaurant_reviews_training.reduced_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.Run and Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T01:07:22.934695",
     "start_time": "2017-01-17T01:07:22.929371"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define models to be tested\n",
    "LR = LogisticRegression(verbose=10, n_jobs=-1)\n",
    "LR2 = LogisticRegression(verbose=10, n_jobs=-1, solver='sag',\n",
    "                        random_state=555, multi_class='multinomial',\n",
    "                        max_iter = 600)\n",
    "SVM = LinearSVC(verbose=10)\n",
    "ExtraTrees = ExtraTreesClassifier(verbose=10,\n",
    "                                  random_state=500,\n",
    "                                  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T02:19:54.628256",
     "start_time": "2017-01-17T01:07:22.936183"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression simple model\n",
      "[LibLinear]Model Logistic Regression simple has finished training\n",
      "The Logistic Regression simple model has an accuracy of 0.8370720063086776 \n",
      "\n",
      "Fitting Logistic Regression multinomial model\n",
      "max_iter reached after 1089 seconds\n",
      "Model Logistic Regression multinomial has finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toavina_andria/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 18.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Logistic Regression multinomial model has an accuracy of 0.8395102249597716 \n",
      "\n",
      "Fitting Support Vector Machines model\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toavina_andria/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py:920: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Support Vector Machines has finished training\n",
      "The Support Vector Machines model has an accuracy of 0.8335660013427253 \n",
      "\n",
      "Fitting Extremely Randomized Trees model\n",
      "building tree 1 of 10building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed: 18.5min remaining: 43.1min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 18.6min remaining: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed: 18.7min remaining:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 18.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Extremely Randomized Trees has finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    0.7s remaining:    1.6s\n",
      "[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed:    0.7s remaining:    0.7s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:    0.7s remaining:    0.3s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Extremely Randomized Trees model has an accuracy of 0.7853600315433882 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run models\n",
    "for name,model in [('Logistic Regression simple',LR),\n",
    "                   ('Logistic Regression multinomial', LR2),\n",
    "                   ('Support Vector Machines', SVM),\n",
    "                   ('Extremely Randomized Trees', ExtraTrees)]:\n",
    "    run_model(name,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T02:44:39.450645",
     "start_time": "2017-01-17T02:24:21.093378"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression 3 model\n",
      "convergence after 522 epochs took 1218 seconds\n",
      "Model Logistic Regression 3 has finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 20.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Logistic Regression 3 model has an accuracy of 0.8395102249597716 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR3 = LogisticRegression(verbose=10, n_jobs=-1, solver='sag',\n",
    "                        random_state=555, multi_class='multinomial',\n",
    "                        max_iter = 2000)\n",
    "run_model('Logistic Regression 3',LR3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Evaluate Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T09:53:05.184160",
     "start_time": "2017-01-17T09:53:04.618198"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74896</td>\n",
       "      <td>7232</td>\n",
       "      <td>11039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13925</td>\n",
       "      <td>19949</td>\n",
       "      <td>30439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4317</td>\n",
       "      <td>8349</td>\n",
       "      <td>299049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1       2\n",
       "0  74896   7232   11039\n",
       "1  13925  19949   30439\n",
       "2   4317   8349  299049"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(BoW_Y_test, LR3.predict(BoW_X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T10:00:10.527276",
     "start_time": "2017-01-17T10:00:10.195769"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              precision    recall  f1-score   support\n",
      "\n",
      "Below average (1 or 2 stars)       0.80      0.80      0.80     93167\n",
      "           Average (3 stars)       0.56      0.31      0.40     64313\n",
      "Above average (4 or 5 stars)       0.88      0.96      0.92    311715\n",
      "\n",
      "                 avg / total       0.82      0.84      0.82    469195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Below average (1 or 2 stars)','Average (3 stars)','Above average (4 or 5 stars)']\n",
    "print(classification_report(BoW_Y_test, LR3.predict(BoW_X_test),target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As may be expected, the average class reviews do not have text discriminative enough to make them sufficiently distinctive between truly mediocre and above average restaurants. \n",
    "\n",
    "However, from a product perspective it may make sense to keep 3 classees to differentiate between restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T09:46:22.770189",
     "start_time": "2017-01-17T09:46:22.767677"
    }
   },
   "source": [
    "## e. Potential Refinements with more time and resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Increase number of samples\n",
    "* Perform grid search on SVM and LR models\n",
    "* Look at other models\n",
    "* Rebalance examples, especially average class\n",
    "* See what performance is with just two classes (suspect accuracy would increase further, but less differentiation)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "746678a6c1714b77b014e451d5ef24b3": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
